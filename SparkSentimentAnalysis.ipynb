{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification of Goodreads Book Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, sql\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"building a warehouse\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true')\\\n",
    "        .load('reviews.csv')\n",
    "        # load('hdfs://localhost/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df = df.withColumn('rating_f', f.round(df.rating.cast('float'), 2))\n",
    "df = df.withColumn('word_count', f.size(f.split(f.col('review'),' ')))\n",
    "df = df.filter(f.col('word_count') > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         genre|count|\n",
      "+--------------+-----+\n",
      "|    Nonfiction|  356|\n",
      "|       Fiction|  120|\n",
      "|    Historical|  114|\n",
      "|    Philosophy|   84|\n",
      "|       History|   77|\n",
      "|           Art|   75|\n",
      "|       Mystery|   48|\n",
      "|       Science|   39|\n",
      "|Sequential Art|   30|\n",
      "|      Classics|   24|\n",
      "|     Childrens|   21|\n",
      "| Autobiography|   19|\n",
      "|     Christian|   18|\n",
      "|    Psychology|   17|\n",
      "|      Politics|   15|\n",
      "|   Young Adult|   14|\n",
      "|       Romance|   14|\n",
      "|     Biography|   13|\n",
      "|      Business|   10|\n",
      "|       Fantasy|    7|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('genre').count().orderBy(f.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiments for data\n",
    "df=df.withColumn('label',f.when(df['rating_f']<3.7,-1).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classes for data\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "splits = [float('-inf'), 3.50,\n",
    "          3.51, 3.75,\n",
    "          3.76, 4.00,\n",
    "          4.01, 4.25,\n",
    "          4.26, 4.50,\n",
    "          4.51, 4.75,\n",
    "          4.76, float('inf')]\n",
    "labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "b=Bucketizer(splits=splits,inputCol='rating_f',outputCol='b_label')\n",
    "dfs = b.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select(['review', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, \\\n",
    "    HashingTF, IDF, CountVectorizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stop_words = ['ourselves', 'hers', 'between', 'yourself',\n",
    "              'but', 'again', 'there', 'about', 'once', \n",
    "              'during', 'out', 'very', 'having', 'with',\n",
    "              'they', 'own', 'an', 'be', 'some', 'for',\n",
    "              'do', 'its', 'yours', 'such', 'into', 'of',\n",
    "              'most', 'itself', 'other', 'off', 'is', 's',\n",
    "              'am', 'or', 'who', 'as', 'from', 'him',\n",
    "              'each', 'the', 'themselves', 'until', 'below',\n",
    "              'are', 'we', 'these', 'your', 'his', 'through',\n",
    "              'don', 'nor', 'me', 'were', 'her', 'more',\n",
    "              'himself', 'this', 'down', 'should', 'our',\n",
    "              'their', 'while', 'above', 'both', 'up',\n",
    "              'to', 'ours', 'had', 'she', 'all', 'no',\n",
    "              'when', 'at', 'any', 'before', 'them', 'same',\n",
    "              'and', 'been', 'have', 'in', 'will', 'on',\n",
    "              'does', 'yourselves', 'then', 'that', 'because',\n",
    "              'what', 'over', 'why', 'so', 'can', 'did', 'not',\n",
    "              'now', 'under', 'he', 'you', 'herself', 'has',\n",
    "              'just', 'where', 'too', 'only', 'myself', 'which',\n",
    "              'those', 'i', 'after', 'few', 'whom', 't',\n",
    "              'being', 'if', 'theirs', 'my', 'against', 'a',\n",
    "              'by', 'doing', 'it', 'how', 'further', 'was',\n",
    "              'here', 'than']\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\",\n",
    "                                    outputCol=\"filtered\")\\\n",
    "                                    .setStopWords(stop_words)\n",
    "hashingTF = HashingTF(inputCol=\"filtered\",\n",
    "                      outputCol=\"rawFeatures\",\n",
    "                      numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "# countVectors = CountVectorizer(inputCol=\"filtered\",\n",
    "#                                outputCol=\"features\",\n",
    "#                                vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover,\n",
    "                            hashingTF, idf])\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 898\n",
      "Test Dataset Count: 250\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "(trainingData, testData) = dataset.randomSplit([0.75, 0.25], 1234)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.8276556291390728\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.85, 1.0, 1.25])\n",
    "             .build())\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "# Create 10-fold CrossValidator\n",
    "cv = CrossValidator(estimator=nb, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "\n",
    "cvModel = cv.fit(trainingData)\n",
    "predictions = cvModel.transform(testData)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
